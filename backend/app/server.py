#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse, StreamingResponse
import json
import asyncio

# langserve imports
from langserve import add_routes

from .agent import ChatAgent
from .config import get_settings

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="AI Agent API (LangServe + OpenAI Compatible)",
    description="åŸºäºLangServeçš„AI AgentèŠå¤©æœåŠ¡ï¼Œå®Œå…¨å…¼å®¹OpenAI APIåè®®",
    version="1.0.0",
    docs_url=None,  # ç¦ç”¨æ–‡æ¡£ä»¥é¿å…OpenAPI schemaé—®é¢˜
    redoc_url=None,
    servers=[
        {"url": "http://localhost:8000", "description": "æœ¬åœ°å¼€å‘æœåŠ¡å™¨"},
    ]
)

# é…ç½®CORS
settings = get_settings()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å¼€å‘ç¯å¢ƒå…è®¸æ‰€æœ‰æ¥æº
    allow_credentials=False,  # å½“allow_originsä¸º*æ—¶ï¼Œå¿…é¡»è®¾ä¸ºFalse
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆ›å»ºèŠå¤©ä»£ç†å®ä¾‹
print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– AI Agent...")
try:
    chat_agent = ChatAgent()
    print("âœ… AI Agent åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    print(f"âŒ AI Agent åˆå§‹åŒ–å¤±è´¥: {e}")
    chat_agent = None

# è·å–LangChainé“¾
chain = chat_agent.get_chain() if chat_agent else None

# ä½¿ç”¨langserveæ·»åŠ OpenAIå…¼å®¹çš„è·¯ç”±
if chain:
    print("ğŸ”— æ­£åœ¨æ·»åŠ  LangServe è·¯ç”±...")
    
    # æ·»åŠ èŠå¤©å®Œæˆç«¯ç‚¹ (ç®€åŒ–é…ç½®)
    add_routes(
        app,
        chain,
        path="/v1/chat/completions",
        enable_feedback_endpoint=False,
        enable_public_trace_link_endpoint=False
    )
    
    print("âœ… LangServe è·¯ç”±æ·»åŠ æˆåŠŸ")
else:
    print("âŒ æ— æ³•æ·»åŠ  LangServe è·¯ç”±ï¼ŒAgent åˆå§‹åŒ–å¤±è´¥")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ä¿¡æ¯"""
    return {
        "status": "ok",
        "service": "AI Agent Backend (LangServe)",
        "version": "1.0.0",
        "endpoints": {
            "health": "/health", 
            "models": "/v1/models",
            "chat": "/v1/chat/completions"
        }
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy",
        "service": "AI Agent Backend (LangServe)",
        "version": "1.0.0",
        "framework": "LangChain + LangServe",
        "agent_available": chat_agent is not None,
        "provider": chat_agent.get_model_info()["provider"] if chat_agent else "unknown",
        "model": chat_agent.get_model_info()["model"] if chat_agent else "unknown"
    }

async def generate_stream_response(user_input: str, model_info: dict, tools: list = None):
    """ç”Ÿæˆæµå¼å“åº”ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨"""
    try:
        chat_id = f"chatcmpl-{__import__('uuid').uuid4().hex[:8]}"
        created_time = int(__import__('time').time())
        
        # å‘é€å¼€å§‹chunk
        yield f"data: {json.dumps({'id': chat_id, 'object': 'chat.completion.chunk', 'created': created_time, 'model': model_info['model'], 'choices': [{'index': 0, 'delta': {'role': 'assistant'}, 'finish_reason': None}]})}\n\n"
        
        # è°ƒç”¨agenté“¾
        chain = chat_agent.get_chain()
        result = await chain.ainvoke({"input": user_input})
        
        response_content = result.get("output", "")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ï¼ˆé€šè¿‡æ£€æŸ¥agentçš„ä¸­é—´æ­¥éª¤ï¼‰
        intermediate_steps = result.get("intermediate_steps", [])
        
        if intermediate_steps:
            # æœ‰å·¥å…·è°ƒç”¨ï¼Œå‘é€å·¥å…·è°ƒç”¨chunks
            for step in intermediate_steps:
                action, observation = step
                if hasattr(action, 'tool') and hasattr(action, 'tool_input'):
                    # å‘é€å·¥å…·è°ƒç”¨chunk
                    tool_call_chunk = {
                        "id": chat_id,
                        "object": "chat.completion.chunk",
                        "created": created_time,
                        "model": model_info["model"],
                        "choices": [{
                            "index": 0,
                            "delta": {
                                "tool_calls": [{
                                    "index": 0,
                                    "id": f"call_{__import__('uuid').uuid4().hex[:8]}",
                                    "type": "function",
                                    "function": {
                                        "name": action.tool,
                                        "arguments": json.dumps(action.tool_input)
                                    }
                                }]
                            },
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(tool_call_chunk)}\n\n"
                    await asyncio.sleep(0.05)
        
        # å‘é€å“åº”å†…å®¹chunks
        if response_content:
            # æŒ‰å­—ç¬¦åˆ†å‰²å®ç°çœŸæ­£çš„æµå¼æ•ˆæœ
            for i, char in enumerate(response_content):
                chunk_data = {
                    "id": chat_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": model_info["model"],
                    "choices": [{
                        "index": 0,
                        "delta": {"content": char},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(chunk_data)}\n\n"
                await asyncio.sleep(0.01)  # æ‰“å­—æ•ˆæœ
        
        # å‘é€ç»“æŸchunk
        end_chunk = {
            "id": chat_id,
            "object": "chat.completion.chunk",
            "created": created_time,
            "model": model_info["model"],
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "tool_calls" if intermediate_steps else "stop"
            }]
        }
        yield f"data: {json.dumps(end_chunk)}\n\n"
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        print(f"âŒ æµå¼å“åº”ç”Ÿæˆé”™è¯¯: {e}")
        error_chunk = {
            "id": f"chatcmpl-{__import__('uuid').uuid4().hex[:8]}",
            "object": "chat.completion.chunk",
            "created": int(__import__('time').time()),
            "model": model_info.get("model", "unknown"),
            "choices": [{
                "index": 0,
                "delta": {"content": f"é”™è¯¯: {str(e)}"},
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(error_chunk)}\n\n"
        yield "data: [DONE]\n\n"

@app.post("/v1/chat/completions")
async def openai_chat_completions(request: dict):
    """OpenAIå…¼å®¹çš„èŠå¤©å®Œæˆç«¯ç‚¹ï¼Œæ”¯æŒæµå¼å’Œéæµå¼"""
    if not chat_agent:
        return {"error": "Agent not initialized"}
    
    try:
        # æå–å‚æ•°
        messages = request.get("messages", [])
        if not messages:
            return {"error": "Messages are required"}
        
        stream = request.get("stream", False)
        user_input = messages[-1].get("content", "")
        model_info = chat_agent.get_model_info()
        
        # æµå¼å“åº”
        if stream:
            tools = request.get("tools", [])
            return StreamingResponse(
                generate_stream_response(user_input, model_info, tools),
                media_type="text/event-stream"
            )
        
        # éæµå¼å“åº”
        chain = chat_agent.get_chain()
        result = await chain.ainvoke({"input": user_input})
        
        return {
            "id": f"chatcmpl-{__import__('uuid').uuid4().hex[:8]}",
            "object": "chat.completion",
            "created": int(__import__('time').time()),
            "model": model_info["model"],
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": result.get("output", "")
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }
        
    except Exception as e:
        print(f"âŒ èŠå¤©å®Œæˆé”™è¯¯: {e}")
        return {"error": f"èŠå¤©å®Œæˆå¤±è´¥: {str(e)}"}

@app.get("/v1/models")
async def list_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ (OpenAIå…¼å®¹)"""
    if not chat_agent:
        return {"error": "Agent not initialized"}
    
    model_info = chat_agent.get_model_info()
    provider = model_info["provider"]
    model_name = model_info["model"]
    
    # è¿”å›OpenAIå…¼å®¹çš„æ¨¡å‹åˆ—è¡¨
    return {
        "object": "list",
        "data": [
            {
                "id": model_name,
                "object": "model",
                "created": 1677610602,
                "owned_by": provider,
                "permission": [],
                "root": model_name,
                "parent": None,
            }
        ]
    }



# ä¸­é—´ä»¶ï¼šæ·»åŠ LangServeç›¸å…³çš„å¤´éƒ¨ä¿¡æ¯
@app.middleware("http")
async def add_langserve_headers(request: Request, call_next):
    """æ·»åŠ LangServeç›¸å…³å¤´éƒ¨ä¿¡æ¯"""
    response = await call_next(request)
    
    # æ·»åŠ æœåŠ¡è¯†åˆ«å¤´éƒ¨
    response.headers["X-Powered-By"] = "LangServe"
    response.headers["X-Agent-Version"] = "1.0.0"
    
    return response

# å¯¼å‡ºåº”ç”¨å®ä¾‹
def create_app():
    """åˆ›å»ºåº”ç”¨å®ä¾‹"""
    return app 