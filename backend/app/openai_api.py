#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import time
import uuid
from typing import List, Dict, Any, Optional, AsyncGenerator, Union
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
import json
import asyncio

from .agent import ChatAgent

# Function Calling ç›¸å…³æ•°æ®æ¨¡å‹
class FunctionParameter(BaseModel):
    type: str = Field(..., description="å‚æ•°ç±»å‹")
    description: Optional[str] = Field(None, description="å‚æ•°æè¿°")
    enum: Optional[List[str]] = Field(None, description="æšä¸¾å€¼")

class FunctionParameters(BaseModel):
    type: str = Field(default="object", description="å‚æ•°ç±»å‹")
    properties: Dict[str, FunctionParameter] = Field(..., description="å‚æ•°å±æ€§")
    required: List[str] = Field(default_factory=list, description="å¿…éœ€å‚æ•°åˆ—è¡¨")

class Function(BaseModel):
    name: str = Field(..., description="å‡½æ•°åç§°")
    description: Optional[str] = Field(None, description="å‡½æ•°æè¿°")
    parameters: Optional[FunctionParameters] = Field(None, description="å‡½æ•°å‚æ•°")

class Tool(BaseModel):
    type: str = Field(default="function", description="å·¥å…·ç±»å‹")
    function: Function = Field(..., description="å‡½æ•°å®šä¹‰")

class FunctionCall(BaseModel):
    name: str = Field(..., description="è°ƒç”¨çš„å‡½æ•°å")
    arguments: str = Field(..., description="å‡½æ•°å‚æ•°JSONå­—ç¬¦ä¸²")

class ToolCall(BaseModel):
    id: str = Field(..., description="å·¥å…·è°ƒç”¨ID")
    type: str = Field(default="function", description="å·¥å…·ç±»å‹")
    function: FunctionCall = Field(..., description="å‡½æ•°è°ƒç”¨")

# OpenAI API å…¼å®¹çš„æ•°æ®æ¨¡å‹
class ChatMessage(BaseModel):
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: system, user, assistant, tool")
    content: Optional[str] = Field(None, description="æ¶ˆæ¯å†…å®¹")
    name: Optional[str] = Field(None, description="æ¶ˆæ¯åç§°")
    tool_call_id: Optional[str] = Field(None, description="å·¥å…·è°ƒç”¨ID")
    tool_calls: Optional[List[ToolCall]] = Field(None, description="å·¥å…·è°ƒç”¨åˆ—è¡¨")

class ChatCompletionRequest(BaseModel):
    model: str = Field(default="deepseek-chat", description="æ¨¡å‹åç§°")
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    temperature: Optional[float] = Field(default=0.7, ge=0, le=2, description="æ¸©åº¦å‚æ•°")
    max_tokens: Optional[int] = Field(default=2000, gt=0, description="æœ€å¤§tokensæ•°")
    stream: Optional[bool] = Field(default=False, description="æ˜¯å¦æµå¼è¿”å›")
    top_p: Optional[float] = Field(default=1.0, ge=0, le=1, description="top_på‚æ•°")
    frequency_penalty: Optional[float] = Field(default=0, ge=-2, le=2, description="é¢‘ç‡æƒ©ç½š")
    presence_penalty: Optional[float] = Field(default=0, ge=-2, le=2, description="å­˜åœ¨æƒ©ç½š")
    user: Optional[str] = Field(default=None, description="ç”¨æˆ·æ ‡è¯†")
    # Function Calling æ”¯æŒ
    functions: Optional[List[Function]] = Field(None, description="å¯ç”¨å‡½æ•°åˆ—è¡¨(legacy)")
    function_call: Optional[Union[str, Dict[str, str]]] = Field(None, description="å‡½æ•°è°ƒç”¨æ§åˆ¶(legacy)")
    tools: Optional[List[Tool]] = Field(None, description="å¯ç”¨å·¥å…·åˆ—è¡¨")
    tool_choice: Optional[Union[str, Dict[str, Any]]] = Field(None, description="å·¥å…·é€‰æ‹©ç­–ç•¥")

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: str = "stop"

class ChatCompletionUsage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: ChatCompletionUsage

# æµå¼å“åº”çš„æ•°æ®æ¨¡å‹
class ChatCompletionStreamChoice(BaseModel):
    index: int
    delta: Dict[str, Any]
    finish_reason: Optional[str] = None

class ChatCompletionStreamResponse(BaseModel):
    id: str
    object: str = "chat.completion.chunk"
    created: int
    model: str
    choices: List[ChatCompletionStreamChoice]

# æ¨¡å‹ä¿¡æ¯
class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "deepseek"

class ModelsResponse(BaseModel):
    object: str = "list"
    data: List[ModelInfo]

# åˆ›å»ºOpenAIå…¼å®¹çš„è·¯ç”±å™¨
openai_router = APIRouter(prefix="/v1")

# å…¨å±€agentå®ä¾‹ (å°†åœ¨server.pyä¸­åˆå§‹åŒ–)
chat_agent: Optional[ChatAgent] = None

def set_chat_agent(agent: ChatAgent):
    """è®¾ç½®å…¨å±€chat agentå®ä¾‹"""
    global chat_agent
    chat_agent = agent

@openai_router.get("/models", response_model=ModelsResponse)
async def list_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ (OpenAIå…¼å®¹)"""
    if not chat_agent:
        raise HTTPException(status_code=500, detail="Chat agent æœªåˆå§‹åŒ–")
    
    # è·å–å½“å‰æ¨¡å‹ä¿¡æ¯
    model_info = chat_agent.get_model_info()
    current_provider = model_info["provider"]
    
    # æ ¹æ®å½“å‰æä¾›å•†è¿”å›ç›¸åº”çš„æ¨¡å‹åˆ—è¡¨
    models = []
    
    if current_provider == "deepseek":
        models = [
            ModelInfo(
                id="deepseek-chat",
                created=int(time.time()),
                owned_by="deepseek"
            ),
            ModelInfo(
                id="deepseek-reasoner", 
                created=int(time.time()),
                owned_by="deepseek"
            )
        ]
    elif current_provider == "gemini":
        models = [
            ModelInfo(
                id="gemini-2.0-flash-exp",
                created=int(time.time()),
                owned_by="google"
            ),
            ModelInfo(
                id="gemini-1.5-pro",
                created=int(time.time()),
                owned_by="google"
            ),
            ModelInfo(
                id="gemini-1.5-flash",
                created=int(time.time()),
                owned_by="google"
            )
        ]
    elif current_provider == "openai":
        models = [
            ModelInfo(
                id="gpt-4",
                created=int(time.time()),
                owned_by="openai"
            ),
            ModelInfo(
                id="gpt-4-turbo",
                created=int(time.time()),
                owned_by="openai"
            ),
            ModelInfo(
                id="gpt-3.5-turbo",
                created=int(time.time()),
                owned_by="openai"
            )
        ]
    else:
        # é»˜è®¤è¿”å›é€šç”¨æ¨¡å‹åˆ—è¡¨
        models = [
            ModelInfo(
                id=model_info["model"],
                created=int(time.time()),
                owned_by=current_provider
            )
        ]
    
    return ModelsResponse(data=models)

@openai_router.get("/model/current")
async def get_current_model():
    """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯"""
    if not chat_agent:
        raise HTTPException(status_code=500, detail="Chat agent æœªåˆå§‹åŒ–")
    
    model_info = chat_agent.get_model_info()
    memory_stats = chat_agent.get_memory_stats()
    
    return {
        "current_model": model_info,
        "memory_stats": memory_stats,
        "timestamp": int(time.time())
    }

@openai_router.post("/model/switch")
async def switch_model(request: dict):
    """åˆ‡æ¢æ¨¡å‹æä¾›å•†å’Œæ¨¡å‹"""
    if not chat_agent:
        raise HTTPException(status_code=500, detail="Chat agent æœªåˆå§‹åŒ–")
    
    provider = request.get("provider")
    api_key = request.get("api_key")
    model = request.get("model")
    
    if not provider:
        raise HTTPException(status_code=400, detail="å¿…é¡»æŒ‡å®šæ¨¡å‹æä¾›å•†")
    
    try:
        success = chat_agent.switch_model(provider, api_key, model)
        
        if success:
            new_model_info = chat_agent.get_model_info()
            return {
                "success": True,
                "message": f"æˆåŠŸåˆ‡æ¢åˆ° {provider.upper()}",
                "new_model": new_model_info
            }
        else:
            raise HTTPException(status_code=500, detail="æ¨¡å‹åˆ‡æ¢å¤±è´¥")
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¨¡å‹åˆ‡æ¢é”™è¯¯: {str(e)}")

def estimate_tokens(text: str) -> int:
    """ç®€å•çš„tokenä¼°ç®— (1ä¸ªæ±‰å­—â‰ˆ1.5tokens, 1ä¸ªè‹±æ–‡å•è¯â‰ˆ1token)"""
    chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
    english_words = len([w for w in text.split() if w.isalpha()])
    other_chars = len(text) - chinese_chars - sum(len(w) for w in text.split() if w.isalpha())
    return int(chinese_chars * 1.5 + english_words + other_chars * 0.5)

async def stream_chat_completion(
    request: ChatCompletionRequest,
    completion_id: str
) -> AsyncGenerator[str, None]:
    """æµå¼èŠå¤©å®Œæˆç”Ÿæˆå™¨"""
    try:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«function calling
        has_functions = bool(request.functions or request.tools)
        
        if has_functions:
            # å¯¹äºfunction callingï¼Œæš‚æ—¶ä¸æ”¯æŒæµå¼
            # å› ä¸ºéœ€è¦ç­‰å¾…æ¨¡å‹å†³å®šæ˜¯å¦è°ƒç”¨å‡½æ•°
            yield f"data: {json.dumps({'error': 'Function calling æš‚ä¸æ”¯æŒæµå¼æ¨¡å¼'})}\n\n"
            return
        
        # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_message = ""
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_message = msg.content
                break
        
        if not user_message:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")
        
        # è°ƒç”¨agentè·å–å®Œæ•´å›å¤
        full_response = await chat_agent.process_message(user_message)
        
        # æ¨¡æ‹Ÿæµå¼è¿”å› (é€å­—ç¬¦å‘é€)
        current_content = ""
        
        # å‘é€å¼€å§‹æ ‡è®°
        start_chunk = ChatCompletionStreamResponse(
            id=completion_id,
            created=int(time.time()),
            model=request.model,
            choices=[ChatCompletionStreamChoice(
                index=0,
                delta={"role": "assistant"},
                finish_reason=None
            )]
        )
        yield f"data: {start_chunk.model_dump_json()}\n\n"
        
        # é€å­—ç¬¦å‘é€å†…å®¹
        for i, char in enumerate(full_response):
            current_content += char
            
            # å‘é€æ¯ä¸ªå­—ç¬¦
            delta = {"content": char}
            
            chunk_data = ChatCompletionStreamResponse(
                id=completion_id,
                created=int(time.time()),
                model=request.model,
                choices=[ChatCompletionStreamChoice(
                    index=0,
                    delta=delta,
                    finish_reason=None if i < len(full_response) - 1 else "stop"
                )]
            )
            
            yield f"data: {chunk_data.model_dump_json()}\n\n"
            
            # åœ¨æ ‡ç‚¹ç¬¦å·åç¨å¾®å»¶è¿Ÿï¼Œè®©ç”¨æˆ·çœ‹åˆ°æ‰“å­—æ•ˆæœ
            if char in 'ï¼Œã€‚ï¼ï¼Ÿ,.:;!? ':
                await asyncio.sleep(0.1)  # æ ‡ç‚¹åç¨é•¿å»¶è¿Ÿ
            else:
                await asyncio.sleep(0.03)  # æ™®é€šå­—ç¬¦è¾ƒçŸ­å»¶è¿Ÿ
        
        # å‘é€ç»“æŸæ ‡è®°
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        error_chunk = {
            "id": completion_id,
            "object": "chat.completion.chunk", 
            "created": int(time.time()),
            "model": request.model,
            "choices": [{"index": 0, "delta": {}, "finish_reason": "error"}],
            "error": {"message": str(e)}
        }
        yield f"data: {json.dumps(error_chunk)}\n\n"

@openai_router.options("/chat/completions/stream")
async def options_chat_completion_stream():
    """å¤„ç†CORSé¢„æ£€è¯·æ±‚"""
    from fastapi import Response
    response = Response()
    response.headers["Access-Control-Allow-Origin"] = "*"
    response.headers["Access-Control-Allow-Methods"] = "POST, OPTIONS"
    response.headers["Access-Control-Allow-Headers"] = "*"
    response.headers["Access-Control-Max-Age"] = "86400"
    return response

@openai_router.post("/chat/completions/stream")
async def create_chat_completion_stream(request: ChatCompletionRequest):
    """ä¸“é—¨çš„SSEæµå¼èŠå¤©ç«¯ç‚¹"""
    if not chat_agent:
        raise HTTPException(status_code=500, detail="Chat agent æœªåˆå§‹åŒ–")
    
    # å¼ºåˆ¶å¯ç”¨æµå¼æ¨¡å¼
    request.stream = True
    completion_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
    
    return StreamingResponse(
        stream_chat_completion(request, completion_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
            "Access-Control-Allow-Methods": "*"
        }
    )

@openai_router.post("/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest):
    """åˆ›å»ºèŠå¤©å®Œæˆ (OpenAIå…¼å®¹ï¼Œæ”¯æŒFunction Calling)"""
    if not chat_agent:
        raise HTTPException(status_code=500, detail="Chat agent æœªåˆå§‹åŒ–")
    
    completion_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
    created_time = int(time.time())
    
    try:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«function calling
        has_functions = bool(request.functions or request.tools)
        
        if has_functions:
            print(f"ğŸ”§ [Function Calling] æ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨è¯·æ±‚")
            print(f"ğŸ“‹ [Function Calling] è¯·æ±‚æ•°æ®: {request.model_dump_json(indent=2)}")
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            messages = []
            for msg in request.messages:
                message_dict = {"role": msg.role}
                if msg.content:
                    message_dict["content"] = msg.content
                if msg.name:
                    message_dict["name"] = msg.name
                if msg.tool_call_id:
                    message_dict["tool_call_id"] = msg.tool_call_id
                if msg.tool_calls:
                    message_dict["tool_calls"] = [
                        {
                            "id": tc.id,
                            "type": tc.type,
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments
                            }
                        }
                        for tc in msg.tool_calls
                    ]
                messages.append(message_dict)
            
            # è½¬æ¢functionsæ ¼å¼ (legacyæ”¯æŒ)
            functions = None
            if request.functions:
                functions = []
                for f in request.functions:
                    func_dict = {
                        "name": f.name,
                        "description": f.description,
                    }
                    
                    # å¤„ç†parameters
                    if f.parameters:
                        params = f.parameters.model_dump()
                        # ç¡®ä¿requiredå­—æ®µæ˜¯æ•°ç»„è€Œä¸æ˜¯None
                        if "required" in params and params["required"] is None:
                            params["required"] = []
                        
                        # æ¸…ç†propertiesä¸­çš„Noneå€¼
                        if "properties" in params:
                            cleaned_properties = {}
                            for prop_name, prop_value in params["properties"].items():
                                cleaned_prop = {k: v for k, v in prop_value.items() if v is not None}
                                cleaned_properties[prop_name] = cleaned_prop
                            params["properties"] = cleaned_properties
                        
                        func_dict["parameters"] = params
                    else:
                        func_dict["parameters"] = {
                            "type": "object",
                            "properties": {},
                            "required": []
                        }
                    
                    functions.append(func_dict)
            
            # è½¬æ¢toolsæ ¼å¼
            tools = None
            if request.tools:
                tools = []
                for t in request.tools:
                    tool_dict = {
                        "type": t.type,
                        "function": {
                            "name": t.function.name,
                            "description": t.function.description,
                        }
                    }
                    
                    # å¤„ç†parameters
                    if t.function.parameters:
                        params = t.function.parameters.model_dump()
                        # ç¡®ä¿requiredå­—æ®µæ˜¯æ•°ç»„è€Œä¸æ˜¯None
                        if "required" in params and params["required"] is None:
                            params["required"] = []
                        
                        # æ¸…ç†propertiesä¸­çš„Noneå€¼
                        if "properties" in params:
                            cleaned_properties = {}
                            for prop_name, prop_value in params["properties"].items():
                                cleaned_prop = {k: v for k, v in prop_value.items() if v is not None}
                                cleaned_properties[prop_name] = cleaned_prop
                            params["properties"] = cleaned_properties
                        
                        tool_dict["function"]["parameters"] = params
                    else:
                        tool_dict["function"]["parameters"] = {
                            "type": "object",
                            "properties": {},
                            "required": []
                        }
                    
                    tools.append(tool_dict)
            
            # è°ƒç”¨æ”¯æŒfunction callingçš„æ–¹æ³•
            result = await chat_agent.process_message_with_functions(
                messages=messages,
                functions=functions,
                tools=tools,
                function_call=request.function_call,
                tool_choice=request.tool_choice,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                stream=request.stream
            )
            
            # è½¬æ¢å“åº”æ ¼å¼
            response_choice = result["choices"][0]
            
            # æ„å»ºChatMessage
            response_message = ChatMessage(
                role=response_choice["message"]["role"],
                content=response_choice["message"].get("content")
            )
            
            # å¤„ç†tool_calls
            if "tool_calls" in response_choice["message"]:
                tool_calls = []
                for tc in response_choice["message"]["tool_calls"]:
                    tool_calls.append(ToolCall(
                        id=tc["id"],
                        type=tc["type"],
                        function=FunctionCall(
                            name=tc["function"]["name"],
                            arguments=tc["function"]["arguments"]
                        )
                    ))
                response_message.tool_calls = tool_calls
            
            response = ChatCompletionResponse(
                id=completion_id,
                created=created_time,
                model=request.model,
                choices=[ChatCompletionChoice(
                    index=0,
                    message=response_message,
                    finish_reason=response_choice.get("finish_reason", "stop")
                )],
                usage=ChatCompletionUsage(
                    prompt_tokens=result["usage"]["prompt_tokens"],
                    completion_tokens=result["usage"]["completion_tokens"],
                    total_tokens=result["usage"]["total_tokens"]
                )
            )
            
            print(f"âœ… [Function Calling] è¿”å›å“åº”")
            return response
        
        # æ™®é€šèŠå¤©å¤„ç†é€»è¾‘
        # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_message = ""
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_message = msg.content
                break
        
        if not user_message:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")
        
        print(f"ğŸ”¥ [OpenAI API] æ”¶åˆ°èŠå¤©è¯·æ±‚: {user_message[:50]}...")
        
        # å¦‚æœæ˜¯æµå¼è¯·æ±‚
        if request.stream:
            return StreamingResponse(
                stream_chat_completion(request, completion_id),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Content-Type": "text/event-stream",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "*",
                    "Access-Control-Allow-Methods": "*"
                }
            )
        
        # éæµå¼è¯·æ±‚ï¼šè°ƒç”¨LangChain agent
        ai_response = await chat_agent.process_message(user_message)
        
        # è®¡ç®—tokenä½¿ç”¨é‡
        prompt_text = " ".join([msg.content or "" for msg in request.messages])
        prompt_tokens = estimate_tokens(prompt_text)
        completion_tokens = estimate_tokens(ai_response)
        total_tokens = prompt_tokens + completion_tokens
        
        response = ChatCompletionResponse(
            id=completion_id,
            created=created_time,
            model=request.model,
            choices=[ChatCompletionChoice(
                index=0,
                message=ChatMessage(role="assistant", content=ai_response),
                finish_reason="stop"
            )],
            usage=ChatCompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens
            )
        )
        
        print(f"âœ… [OpenAI API] è¿”å›å›å¤: {ai_response[:50]}...")
        return response
        
    except Exception as e:
        print(f"âŒ [OpenAI API] å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")

# å…¼å®¹æ€§ç«¯ç‚¹
@openai_router.get("/")
async def openai_root():
    """OpenAI API æ ¹ç«¯ç‚¹"""
    return {
        "message": "OpenAI Compatible API",
        "service": "AI Agent Backend", 
        "version": "0.1.0",
        "framework": "LangChain + DeepSeek"
    } 