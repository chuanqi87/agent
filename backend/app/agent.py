#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
from typing import List, Dict, Any, Optional, Union
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain.chains import ConversationChain
from langchain.prompts import (
    ChatPromptTemplate, 
    MessagesPlaceholder, 
    SystemMessagePromptTemplate, 
    HumanMessagePromptTemplate
)
from fastapi import HTTPException
from .config import get_settings

class ChatAgent:
    """åŸºäºLangChainçš„èŠå¤©ä»£ç†ï¼Œæ”¯æŒFunction Callingå’Œå¤šæ¨¡å‹åˆ‡æ¢"""
    
    def __init__(self):
        """åˆå§‹åŒ–èŠå¤©ä»£ç†"""
        self.settings = get_settings()
        self.model_config = self.settings.get_active_model_config()
        self.setup_llm()
        self.setup_memory()
        self.setup_prompt_template()
        self.setup_conversation_chain()
    
    def setup_llm(self):
        """è®¾ç½®LangChain LLMï¼Œæ”¯æŒå¤šç§æ¨¡å‹"""
        if not self.model_config["api_key"]:
            raise ValueError(f"è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®{self.model_config['provider'].upper()}_API_KEY")
        
        # æ ¹æ®æ¨¡å‹æä¾›å•†è°ƒæ•´é…ç½®
        provider = self.model_config["provider"]
        model_name = self.model_config["model"]
        
        # å¯¹äºGeminiï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if provider == "gemini":
            # Geminié€šè¿‡OpenAIå…¼å®¹æ¥å£è®¿é—®
            base_url = self.model_config["base_url"]
            # å¦‚æœä½¿ç”¨Google AI Studioçš„API
            if "generativelanguage.googleapis.com" in base_url:
                # æ„å»ºå…¼å®¹çš„URL
                base_url = f"{base_url}/openai"
        else:
            base_url = self.model_config["base_url"]
        
        self.llm = ChatOpenAI(
            openai_api_key=self.model_config["api_key"],
            openai_api_base=base_url,
            model_name=model_name,
            temperature=0.7,
            max_tokens=2000,
            streaming=False
        )
        
        print(f"âœ… LangChain + {provider.upper()} LLM åˆå§‹åŒ–æˆåŠŸ")
        print(f"ğŸ“¡ Base URL: {base_url}")
        print(f"ğŸ¤– Model: {model_name}")
    
    def setup_memory(self):
        """è®¾ç½®å¯¹è¯è®°å¿†"""
        self.memory = ConversationBufferWindowMemory(
            k=10,  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
            return_messages=True,
            memory_key="chat_history"
        )
        print(f"ğŸ§  å¯¹è¯è®°å¿†åˆå§‹åŒ–æˆåŠŸ (çª—å£å¤§å°: {self.memory.k})")
    
    def setup_prompt_template(self):
        """è®¾ç½®æç¤ºæ¨¡æ¿"""
        provider = self.model_config["provider"]
        model_name = self.model_config["model"]
        
        system_message = f"""ä½ æ˜¯ä¸€ä¸ªåŒ»ç”Ÿã€‚è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š

1. ç”¨ä¸­æ–‡å›ç­”é—®é¢˜
2. ä¿æŒå‹å¥½å’Œä¸“ä¸šçš„è¯­è°ƒ  
3. å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·è¯šå®è¯´æ˜
4. å°½é‡æä¾›æœ‰ç”¨å’Œå‡†ç¡®çš„ä¿¡æ¯
5. å¯ä»¥è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œè®°ä½ä¹‹å‰çš„ä¸Šä¸‹æ–‡
6. å±•ç°å‡ºæ·±åº¦æ€è€ƒå’Œæ¨ç†èƒ½åŠ›
7. åˆ©ç”¨LangChainçš„èƒ½åŠ›æä¾›æ›´æ™ºèƒ½çš„å›ç­”
8. å¦‚æœæœ‰å¯ç”¨çš„å·¥å…·å‡½æ•°ï¼Œè¯·åˆç†ä½¿ç”¨å®ƒä»¬æ¥å¸®åŠ©ç”¨æˆ·

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›æœ€å¥½çš„å¸®åŠ©ã€‚"""
        
        self.prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_message),
            MessagesPlaceholder(variable_name="chat_history"),
            HumanMessagePromptTemplate.from_template("{input}")
        ])
        print(f"ğŸ“ æç¤ºæ¨¡æ¿è®¾ç½®å®Œæˆ - {provider}")
    
    def setup_conversation_chain(self):
        """è®¾ç½®å¯¹è¯é“¾"""
        self.conversation = ConversationChain(
            llm=self.llm,
            prompt=self.prompt,
            memory=self.memory,
            verbose=True  # å¯ç”¨è¯¦ç»†æ—¥å¿—
        )
        print(f"ğŸ”— å¯¹è¯é“¾åˆå§‹åŒ–å®Œæˆ")
    
    def switch_model(self, provider: str, api_key: str = None, model: str = None):
        """åŠ¨æ€åˆ‡æ¢æ¨¡å‹"""
        try:
            # æ›´æ–°é…ç½®
            if provider.lower() == "deepseek":
                if api_key:
                    self.settings.DEEPSEEK_API_KEY = api_key
                if model:
                    self.settings.DEEPSEEK_MODEL = model
                self.settings.MODEL_PROVIDER = "deepseek"
            elif provider.lower() == "gemini":
                if api_key:
                    self.settings.GEMINI_API_KEY = api_key
                if model:
                    self.settings.GEMINI_MODEL = model
                self.settings.MODEL_PROVIDER = "gemini"
            elif provider.lower() == "openai":
                if api_key:
                    self.settings.OPENAI_API_KEY_ORIGINAL = api_key
                if model:
                    self.settings.OPENAI_MODEL_ORIGINAL = model
                self.settings.MODEL_PROVIDER = "openai"
            
            # é‡æ–°åˆå§‹åŒ–
            self.model_config = self.settings.get_active_model_config()
            self.setup_llm()
            self.setup_prompt_template()
            self.setup_conversation_chain()
            
            print(f"âœ… æˆåŠŸåˆ‡æ¢åˆ° {provider.upper()} æ¨¡å‹")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥: {str(e)}")
            return False
    
    async def process_message(self, user_message: str, **kwargs) -> str:
        """ä½¿ç”¨LangChainå¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›AIå›å¤"""
        try:
            print(f"ğŸ“¤ [LangChain {self.model_config['provider'].upper()}] å¤„ç†ç”¨æˆ·æ¶ˆæ¯: {user_message[:50]}...")
            
            # ä½¿ç”¨LangChainå¯¹è¯é“¾å¤„ç†æ¶ˆæ¯
            response = await self.llm.ainvoke(
                self.prompt.format_messages(
                    chat_history=self.memory.chat_memory.messages,
                    input=user_message
                )
            )
            
            ai_response = response.content
            print(f"ğŸ“¥ [LangChain {self.model_config['provider'].upper()}] æ”¶åˆ°å›å¤: {ai_response[:50]}...")
            
            # æ›´æ–°LangChainè®°å¿†
            self.memory.chat_memory.add_user_message(user_message)
            self.memory.chat_memory.add_ai_message(ai_response)
            
            return ai_response
            
        except Exception as e:
            error_msg = f"âŒ LangChain + {self.model_config['provider'].upper()} å¤„ç†å¤±è´¥: {str(e)}"
            print(error_msg)
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}"
    
    async def process_message_with_functions(
        self, 
        messages: List[Dict[str, Any]], 
        functions: Optional[List[Dict[str, Any]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        function_call: Optional[Union[str, Dict[str, str]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """å¤„ç†å¸¦æœ‰function callingçš„æ¶ˆæ¯"""
        try:
            provider = self.model_config["provider"]
            print(f"ğŸ”§ [Function Calling {provider.upper()}] å¤„ç†æ¶ˆæ¯ - Functions: {len(functions or [])}, Tools: {len(tools or [])}")
            
            # æ„å»ºå‘é€ç»™åº•å±‚æ¨¡å‹çš„è¯·æ±‚
            request_data = {
                "model": self.model_config["model"],
                "messages": messages,
                "temperature": kwargs.get("temperature", 0.7),
                "max_tokens": kwargs.get("max_tokens", 2000),
                "stream": kwargs.get("stream", False)
            }
            
            # æ·»åŠ function callingç›¸å…³å‚æ•°
            if functions:
                request_data["functions"] = functions
                if function_call:
                    request_data["function_call"] = function_call
            
            if tools:
                request_data["tools"] = tools
                if tool_choice:
                    request_data["tool_choice"] = tool_choice
                    
            # æ ¹æ®ä¸åŒæä¾›å•†è°ƒæ•´APIè°ƒç”¨
            import httpx
            
            headers = {
                "Authorization": f"Bearer {self.model_config['api_key']}",
                "Content-Type": "application/json"
            }
            
            # æ„å»ºAPIç«¯ç‚¹
            base_url = self.model_config["base_url"]
            if provider == "gemini" and "generativelanguage.googleapis.com" in base_url:
                # å¯¹äºGoogle AI Studioï¼Œä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼
                api_url = f"{base_url}/openai/chat/completions"
            else:
                api_url = f"{base_url}/chat/completions"
            
            # å‘é€è¯·æ±‚åˆ°åº•å±‚æ¨¡å‹API
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    api_url,
                    headers=headers,
                    json=request_data,
                    timeout=60.0
                )
                
                if response.status_code != 200:
                    raise HTTPException(
                        status_code=response.status_code,
                        detail=f"åº•å±‚{provider.upper()}APIè°ƒç”¨å¤±è´¥: {response.text}"
                    )
                
                result = response.json()
                print(f"âœ… [Function Calling {provider.upper()}] åº•å±‚æ¨¡å‹å“åº”æˆåŠŸ")
                
                return result
                
        except Exception as e:
            error_msg = f"âŒ Function Calling {self.model_config['provider'].upper()} å¤„ç†å¤±è´¥: {str(e)}"
            print(error_msg)
            raise e
    
    async def process_message_with_chain(self, user_message: str) -> str:
        """ä½¿ç”¨å¯¹è¯é“¾å¤„ç†æ¶ˆæ¯ (åŒæ­¥è½¬å¼‚æ­¥åŒ…è£…)"""
        try:
            provider = self.model_config["provider"]
            print(f"ğŸ“¤ [Chain {provider.upper()}] å¤„ç†ç”¨æˆ·æ¶ˆæ¯: {user_message[:50]}...")
            
            # ç›´æ¥ä½¿ç”¨å¯¹è¯é“¾ (LangChainè‡ªåŠ¨ç®¡ç†ä¸€åˆ‡)
            import asyncio
            response = await asyncio.get_event_loop().run_in_executor(
                None, 
                self.conversation.predict,
                user_message
            )
            
            print(f"ğŸ“¥ [Chain {provider.upper()}] æ”¶åˆ°å›å¤: {response[:50]}...")
            return response
            
        except Exception as e:
            error_msg = f"âŒ LangChain Chain {self.model_config['provider'].upper()} å¤„ç†å¤±è´¥: {str(e)}"
            print(error_msg)
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}"
    
    def clear_history(self):
        """æ¸…é™¤LangChainå¯¹è¯å†å²"""
        self.memory.clear()
        print("âœ… LangChainå¯¹è¯å†å²å·²æ¸…é™¤")
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        messages = self.memory.chat_memory.messages
        return {
            "total_messages": len(messages),
            "user_messages": len([m for m in messages if m.type == "human"]),
            "ai_messages": len([m for m in messages if m.type == "ai"]),
            "memory_window": self.memory.k,
            "current_model": self.model_config["model"],
            "current_provider": self.model_config["provider"]
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        return {
            "provider": self.model_config["provider"],
            "model": self.model_config["model"],
            "base_url": self.model_config["base_url"],
            "has_api_key": bool(self.model_config["api_key"])
        }
    
    async def get_conversation_summary(self) -> str:
        """è·å–å¯¹è¯æ‘˜è¦ (ä½¿ç”¨LangChain)"""
        try:
            messages = self.memory.chat_memory.messages
            if not messages:
                return "æš‚æ— å¯¹è¯å†å²"
            
            # æ„å»ºæ‘˜è¦æç¤º
            conversation_text = "\n".join([
                f"{'ç”¨æˆ·' if m.type == 'human' else 'AI'}: {m.content}"
                for m in messages[-10:]  # æœ€è¿‘10æ¡æ¶ˆæ¯
            ])
            
            summary_prompt = f"""è¯·ä¸ºä»¥ä¸‹å¯¹è¯ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼š

{conversation_text}

æ‘˜è¦ï¼š"""
            
            response = await self.llm.ainvoke([HumanMessage(content=summary_prompt)])
            return response.content
            
        except Exception as e:
            return f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {str(e)}" 